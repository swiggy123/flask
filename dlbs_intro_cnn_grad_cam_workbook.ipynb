{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swiggy123/flask/blob/main/dlbs_intro_cnn_grad_cam_workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcy271XRf-6r"
      },
      "source": [
        "# Hands-On Deep Learning Workbook: Training a CNN for Classification and Insights into a Blackbox Model\n",
        "\n",
        "Lecturer: Susanne Suter (susanne.suter@fhnw.ch)\n",
        "\n",
        "This notebook was inspired by https://appliedmldays.org/workshops/machine-learning-for-smart-dummies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before You Start\n",
        "1) Click on \"Copy to Drive\". A Google account is needed for that.\n",
        "\n",
        "2) Select \"Runtime\" -> \"Change runtime type\" -> \"Hardware accelerator\" : \"GPU\"\n",
        "\n",
        "Now you work on your personal copy of the notebook. Let's get started!"
      ],
      "metadata": {
        "id": "orAHO5IIHCOl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxMkO9Hzu60Q"
      },
      "source": [
        "# Gender Recognition using a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPhFAMt_ZJ2M"
      },
      "source": [
        "In this assignment, we will create and tune a convolutional neural network (CNN) that is able to detect the gender of a given face image. That is, we model a classification task. For simplicity, we define our outputs as male or female.\n",
        "\n",
        "You will be gently guided through every step of the CNN predictor creation. Assignment tasks are explicitly marked with <font color='blue'>Task</font>.\n",
        "\n",
        "The ground truth data contains of the labels (answers) inserted into the CNN that are used for training. That is, the ground truth represents the true y values.\n",
        "\n",
        "The selected ground truth data is not perfect, hence, this will allow you to grasp important aspects about the ground truth data generation.\n",
        "\n",
        "During the assignment you can moreover reflect about what effects biases in the selection of ground truth data have on the model prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4wE8OSTYWri"
      },
      "source": [
        "## Importing Data\n",
        "\n",
        "Retrieve the data set used for this exercise. This time, we will import the data for this exercise directly from a git repository. \n",
        "\n",
        "The ownership of the data is: \"Labeled faces in the wild\"\n",
        "http://vis-www.cs.umass.edu/lfw/\n",
        "Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. \n",
        "Labeled Faces in the Wild: A Database for Studying Face Recognition in\n",
        "Unconstrained Environments. University of Massachusetts, Amherst, \n",
        "Technical Report 07-49, October, 2007.\n",
        "\n",
        "The face images are for teaching purposes prepared in a repository splitting male and female faces such that they are already sorted in two directories for our two classes to predict.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkguOK2CYN4b"
      },
      "outputs": [],
      "source": [
        "# Import data for male and female labelled faces (includes independent benchmark data)\n",
        "\n",
        "!rm -fr faces* # removes the data directory in case it already exists\n",
        "!git clone https://github.com/susuter/faces_red.git faces"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltr faces"
      ],
      "metadata": {
        "id": "5LR-gW2vJtmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPg4kd5N9E1j"
      },
      "source": [
        "The loaded data contains three folders with faces images in jpg format. You can explore them by browsing the directories next to this notebook file.\n",
        "\n",
        "* Folder `female` contains images from female personalities\n",
        "* Folder `male` contains images from male personalities\n",
        "* Folder `benchmark` contains independent test images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_dir = \"faces\"\n",
        "m_dir = os.path.join(base_dir, \"male\")\n",
        "f_dir = os.path.join(base_dir, \"female\")\n",
        "\n",
        "print(m_dir, f_dir)"
      ],
      "metadata": {
        "id": "C6iIYH0qJnuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5C2mO7Up5R7"
      },
      "outputs": [],
      "source": [
        "# Show an image of each gender to make sure that the data is correctly loaded \n",
        "# and to get an idea of how the images look like\n",
        "# Feel free to load other images yourself\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from skimage import io\n",
        "\n",
        "img_f = io.imread(\"faces/female/AJ_Cook_0001.jpg\")\n",
        "img_m = io.imread(\"faces/male/AJ_Lamas_0001.jpg\")\n",
        "\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "ax1 = fig.add_subplot(1,2,1)\n",
        "ax1.imshow(img_f)\n",
        "ax1.set_title(\"Example image females\")\n",
        "ax2 = fig.add_subplot(1,2,2)\n",
        "ax2.imshow(img_m)\n",
        "ax2.set_title(\"Example image males\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNDFXftvxNbo"
      },
      "outputs": [],
      "source": [
        "# Shape of the images\n",
        "# (pixel rows, pixel columns, colors)\n",
        "\n",
        "print( \"Shape of female image: \")\n",
        "print( img_f.shape )\n",
        "\n",
        "print( \"Shape of male image: \")\n",
        "print( img_m.shape )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE91T-_1ckKD"
      },
      "outputs": [],
      "source": [
        "# Print the number of source images available for each dataset\n",
        "\n",
        "import glob\n",
        "\n",
        "def print_file_count_in_dir(dir_name, msg=\"\", extension=\".jpg\"):\n",
        "    print(msg, len(glob.glob(os.path.join(dir_name, \"*\" + extension))))\n",
        "\n",
        "print_file_count_in_dir(m_dir, \"# male cases: \")\n",
        "print_file_count_in_dir(f_dir, \"# female cases: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNARykV2qMkF"
      },
      "outputs": [],
      "source": [
        "# method to randomly select n images from the male and female data sources\n",
        "\n",
        "import os, random, shutil\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 7\n",
        "random.seed(seed)\n",
        "\n",
        "def randomly_select_n_images(in_path, out_path, n):\n",
        "  in_files = os.listdir(in_path)\n",
        "  path, dirs, out_files = next(os.walk(out_path))\n",
        "  while len(out_files) < n:    \n",
        "    choice = random.choice(in_files)\n",
        "    src = os.path.join(in_path, choice)\n",
        "    dst = os.path.join(out_path, choice)\n",
        "    if os.path.isfile(src): \n",
        "      shutil.copy(src, dst)\n",
        "    path, dirs, out_files = next(os.walk(out_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r faces/female/selected\n",
        "!rm -r faces/male/selected\n",
        "!mkdir faces/female/selected\n",
        "!mkdir faces/male/selected\n",
        "\n",
        "# you may ignore the message \n",
        "# \"rm: cannot remove ...: No such file or directoy\"\n",
        "# it happens when you execute that cell for the first time"
      ],
      "metadata": {
        "id": "0-Ysup1Qb9Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SGrwj1l9E1l"
      },
      "source": [
        "### Data Selection\n",
        "<font color='blue'>Task</font>: Select an appropriate number of images for both classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8mLoyy-9E1l"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "# choose your own number of randomly selected images\n",
        "n_f = 800\n",
        "n_m = 800\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "f_dir_sel = os.path.join(f_dir, 'selected')\n",
        "m_dir_sel = os.path.join(m_dir, 'selected')\n",
        "\n",
        "randomly_select_n_images(f_dir, f_dir_sel, n_f)\n",
        "randomly_select_n_images(m_dir, m_dir_sel, n_m)\n",
        "\n",
        "# output the number of selected images \n",
        "print_file_count_in_dir(f_dir_sel)\n",
        "print_file_count_in_dir(m_dir_sel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qemTxODd8Tq"
      },
      "source": [
        "## Data Preparation\n",
        "In this part, we want to make sure that the data is correctly formatted.\n",
        "For example, we need each image to be of the same size with respect to the images dimensions (WIDTH and HEIGHT) and the number of color channels (DEPTH). At the same time, we add the prepared image to a list with our desired file format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alw8K_N_TtRY"
      },
      "outputs": [],
      "source": [
        "# Generate the image lists for both classes, female and male\n",
        "import glob\n",
        "\n",
        "image_list_f = glob.glob(os.path.join(f_dir_sel, '*.jpg'))\n",
        "image_list_m = glob.glob(os.path.join(m_dir_sel,'*.jpg'))\n",
        "print(len(image_list_f), len(image_list_m))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smhJVgO_tjJk"
      },
      "outputs": [],
      "source": [
        "# Define the function that pre-processes the images for the training\n",
        "\n",
        "import numpy as np\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "\n",
        "# This function does the resize and adds the image to the training set\n",
        "def img_preprocessing(image_list, label, X_, y_):\n",
        "\n",
        "    i = 0\n",
        "    for image in image_list:\n",
        "    \n",
        "        if i%100 == 0: print(\"pre-processing image \",i,\" ...\")\n",
        "    \n",
        "        img = io.imread(image)\n",
        "\n",
        "        img = np.array(img)\n",
        "\n",
        "        # Resize the image; make sure all images have same size\n",
        "        pr = 250 # pixel rows (HEIGHT)\n",
        "        pc = 250 # pixel columns (WIDTH)\n",
        "        img = resize(img, (pr, pc), anti_aliasing=True)\n",
        "\n",
        "        if i == 0:\n",
        "            WIDTH, HEIGHT, DEPTH = np.array(img).shape\n",
        "            print(\"image size:\",WIDTH,HEIGHT)\n",
        "\n",
        "        # Show the first 5 images\n",
        "        if i < 5:\n",
        "            plt.imshow(img)\n",
        "            plt.show()\n",
        "    \n",
        "        # Append the pre-processed images to the training set\n",
        "        X_.append(img)\n",
        "        y_.append(label)\n",
        "    \n",
        "        i = i+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOKPUOyE9E1m"
      },
      "source": [
        "<font color='blue'>Task</font>: Process your selected data so that it can be used later for model training. Incl. conversion to Numpy arrays. You may use existing functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8h3GodGuzpB"
      },
      "outputs": [],
      "source": [
        "# Initialize the training set\n",
        "X_v0 = [] # images\n",
        "y_v0 = [] # labels\n",
        "\n",
        "# Pre-process images and add them to training set\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "img_preprocessing(image_list_f, [1,0], X_v0, y_v0)\n",
        "img_preprocessing(image_list_m, [0,1], X_v0, y_v0)\n",
        "### END SOLUTION\n",
        "\n",
        "# Change format of the training set from list to numpy array\n",
        "X = np.array(X_v0) # images\n",
        "y = np.array(y_v0) # labels\n",
        "\n",
        "print(\"Training set is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv-gymYi9E1m"
      },
      "source": [
        "### Data and Labels Check\n",
        "<font color='blue'>Task</font>: Control your data selection by displaying 2 images of each class as an image and specifying the associated label. You may use existing functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAbCpvAjwsrJ"
      },
      "outputs": [],
      "source": [
        "# Define a function that shows an image from the image_set\n",
        "def show_img(image_set, i):\n",
        "    img1 = image_set[i, :, :]\n",
        "    plt.imshow(img1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5jCCANvwzaW"
      },
      "outputs": [],
      "source": [
        "# Show example images from the training set and check their labels\n",
        "# Label -> what we are predicting\n",
        "# Label [1 0] -> female\n",
        "# Label [0 1] -> male\n",
        "\n",
        "def show_label(i):\n",
        "  show_img(X,i)\n",
        "  print(\"Label: \",  y[i,:])\n",
        "\n",
        "# Task: select image indices\n",
        "# Hint 1: the index range depends on your chosen number of selected augmented images\n",
        "# Hint 2: the female images have the lower indices since they were added to the list first\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "show_label(10)\n",
        "show_label(510)\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vRt1ogFxMJt"
      },
      "outputs": [],
      "source": [
        "# Print X (images) and y (labels) shapes\n",
        "print(X.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDcjewvuziFu"
      },
      "outputs": [],
      "source": [
        "# Optionally print X (images) and y (labels) values\n",
        "print(\"Images X: \",X)\n",
        "print(\"Labels y: \",y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INtmefX2gNYF"
      },
      "source": [
        "## Split Data into Training Set and Validation Set\n",
        "\n",
        "During the training, the CNN needs certain validation data to compute the loss. What in return makes it possible to update the parameters (weights) of the trained CNN in order to make it perform better. \n",
        "\n",
        "Therefore, we split our ground truth data into a training set and a validation set. Typically, we choose about 20% of the ground truth as validation set. \n",
        "\n",
        "The terminology of the validation set can vary. It is also called test set. \n",
        "\n",
        "### Training Set Split\n",
        "<font color='blue'>Task</font>: Perform the training data set split so that 15% of the data is in the validation set (test set). You may use existing functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxU3vh-DynUT"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data from the preprocessed set into a train and test set\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "print('X_train shape: ',X_train.shape)\n",
        "print('X_validation shape: ',X_validation.shape)\n",
        "print('y_train shape: ',y_train.shape)\n",
        "print('y_validation shape: ',y_validation.shape)\n",
        "\n",
        "print('Train and validation datasets are ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KAb50hkhf_P"
      },
      "source": [
        "## CNN Model Definition\n",
        "In this section, we define the architecture of our CNN model. That means, we define how many and what type of layers the CNN has. We furthermore define other parameters such as the activation function or our regularization (dropout). \n",
        "\n",
        "<font color='blue'>Task</font>: Configure your own CNN model by adjusting the following parameters: \n",
        "* Select suitable activation functions for each layer of the CNN.\n",
        "* Select an appropriate Loss, which is suitable for a classification problem.\n",
        "* Select Stochastic Gradient Descent as the optimizer for backpropagation.\n",
        "You may use existing functions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "def define_model(num_classes,epochs):\n",
        "    # Create the CNN model\n",
        "    model = Sequential()\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    # Task: enter activation functions\n",
        "\n",
        "    # Layer 1 (convolutional plus max pooling)\n",
        "    model.add(Conv2D(4, (5, 5), input_shape=(X.shape[1], X.shape[2], 3), padding='same', activation='relu', kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Layer 2 (convolutional plus max pooling)\n",
        "    model.add(Conv2D(4, (3, 3), activation='relu', padding='same', kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Layer 3 (convolutional plus max pooling)\n",
        "    model.add(Conv2D(4, (3, 3), activation='relu', padding='same', kernel_constraint=MaxNorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Task: Optionally add additional convolutional layers\n",
        "    # ...\n",
        "\n",
        "    # Additional dense layers (fully connected)    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        " \n",
        "    # Task: choose (custome-defined) optimizer\n",
        "    #lrate = 0.005\n",
        "    #decay = lrate/epochs\n",
        "    #sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
        "    #adam = Adam(lr=lrate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay, amsgrad=False)\n",
        "    adam = Adam()\n",
        "    \n",
        "    # Prepares the model and defines the loss and optimizer\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    \n",
        "    ### END SOLUTION\n",
        "    \n",
        "    print(model.summary())\n",
        "    \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Hal81bwZPfhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqcUHagZ9E1n"
      },
      "source": [
        "<font color='blue'>Task</font>: Select a suitable number of epochs. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAevTL_Z8tka"
      },
      "outputs": [],
      "source": [
        "# Define the duration of the training process, which is given in epochs\n",
        "# In each epoch the model learns once the whole dataset\n",
        "\n",
        "### BEGINN SOLUTION\n",
        "\n",
        "# Task: choose a number of epochs that the model should be trained for\n",
        "epochs = 20\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "#Create the model\n",
        "model=define_model(2,epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzAZ6Wud3Pih"
      },
      "source": [
        "## CNN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X25tTYvDzp-n"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history=model.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=epochs, batch_size=32)\n",
        "\n",
        "# Plot accuracy vs epochs\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot cost function vs epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model cost function')\n",
        "plt.ylabel('cost function')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_validation, y_validation, verbose=0)\n",
        "print(\"______________________________\")\n",
        "print(\"Validation set accuracy: %.2f%%\" % (scores[1]*100))\n",
        "print(\"______________________________\")\n",
        "# Save the model to a json file\n",
        "model_json = model.to_json()\n",
        "with open(\"model_faces_v1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save_weights(\"model_faces_v1.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd2n5rh29E1o"
      },
      "source": [
        "<font color='blue'>Task</font>: Perform your training three times and compare the results. What values do you get? How stable is the training? How satisfied are you with your choice of number of epochs? Do the results in the graphs meet your expectations? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Evaluation"
      ],
      "metadata": {
        "id": "7ELkDbg4SIxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-muted')\n",
        "\n",
        "# Predictions on the validation sample\n",
        "y_pred_validation = (model.predict(X_validation)>0.5).astype('int32')\n",
        "\n",
        "# Predictions on the training sample\n",
        "y_pred_train = (model.predict(X_train)>0.5).astype('int32')\n",
        "\n",
        "print('Confusion matrix for the train set:')\n",
        "cm_train = confusion_matrix(y_train[:,0], y_pred_train[:,0])\n",
        "sns.heatmap(cm_train, annot=True, fmt='d')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')\n",
        "\n",
        "print('[[true negatives, false negatives]')\n",
        "print('[false positives, true positives]]')"
      ],
      "metadata": {
        "id": "hVnSq4pAR6v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>Task</font>: What do the true negatives, false positives, false negatives and true positives mean with respect to our binary classification problem of predicting females and males on photos?\n"
      ],
      "metadata": {
        "id": "gZNi88hfIRHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Confusion matrix for the validation set:')\n",
        "cm_validation = confusion_matrix(y_validation[:,0], y_pred_validation[:,0])\n",
        "sns.heatmap(cm_validation, annot=True, fmt='d')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')"
      ],
      "metadata": {
        "id": "adMe0292SX4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az0nHSZoGfDr"
      },
      "source": [
        "## Independent Benkmark Test Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxYXkxDWjoZE"
      },
      "source": [
        "Now, the trained CNN model is evaluated on an independent test data set, which was not used for to calculated the loss during the training.\n",
        "\n",
        "To differentiate from the terminology test set, which was used during the training, we use here the terminology benchmark test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YqR3gcJa3S8"
      },
      "outputs": [],
      "source": [
        "# Prepare the benchmark test set\n",
        "\n",
        "import glob\n",
        "\n",
        "# Generate the image lists for both benchmark classes, female and male\n",
        "# The source data was downloaded previously from the repository\n",
        "image_list_bench_f = glob.glob('faces/benchmark/female/*.jpg')\n",
        "image_list_bench_m = glob.glob('faces/benchmark/male/*.jpg')\n",
        "\n",
        "# Initialize the benchmark set\n",
        "X_bench_v0 = [] # images\n",
        "y_bench_v0 = [] # labels\n",
        "\n",
        "# Pre-process images and add them to benchmark set\n",
        "print(\"pre-processing female images and add them to benchmark set...\")\n",
        "img_preprocessing(image_list_bench_f, [1,0], X_bench_v0, y_bench_v0)\n",
        "print(\"pre-processing male images and add them to benchmark set...\")\n",
        "img_preprocessing(image_list_bench_m, [0,1], X_bench_v0, y_bench_v0)\n",
        "\n",
        "# Change format of the benchmark set from list to numpy array\n",
        "X_bench = np.array(X_bench_v0) # images\n",
        "y_bench = np.array(y_bench_v0) # labels\n",
        "\n",
        "print(\"Benchmark set is ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMqAUeuRtigk"
      },
      "outputs": [],
      "source": [
        "# Predict the images of the benchmark test set\n",
        "p_bench = (model.predict(X_bench)) # probabilities\n",
        "p_bench_c = (p_bench>0.5).astype('int32') # classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okuiF20Svkr7"
      },
      "source": [
        "## Performance of Benchmark Set\n",
        "\n",
        "<font color='blue'>Task</font>: Implement two functions for the manual calculation of precision and recall. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksht9i1anjlO"
      },
      "outputs": [],
      "source": [
        "# note: handling division by zero for readability does not need to be implemented\n",
        "\n",
        "# precision (positive predictive value)\n",
        "### BEGINN SOLUTION\n",
        "def calc_precision(tp,fp):\n",
        "  return (tp)/(tp+fp)\n",
        "### END SOLUTION\n",
        "\n",
        "# recall or sensitivity (true positive rate)\n",
        "### BEGINN SOLUTION\n",
        "def calc_recall(tp,fn):\n",
        "  return (tp)/(tp+fn)\n",
        "### END SOLUTION\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>Task</font>: Calculate the confusion matrix for the benchmark data set and print the true positives etc. Then use your implemented recall and precision functions and verify your results using sklearn.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNPGAOIw_dco"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXRSgtlDnZHm"
      },
      "outputs": [],
      "source": [
        "# Calculate performance\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "y_true = y_bench[:,0]\n",
        "y_pred = p_bench_c[:,0]\n",
        "\n",
        "print('Confusion matrix for the benchmark test set:')\n",
        "cf = confusion_matrix(y_true, y_pred)\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "print('[[true negatives (tn), false positives (fp)]')\n",
        "print('[false negatives (fn), true positives (tn)]]')\n",
        "print('[[true males (tn), false females (fp)]')\n",
        "print('[false males (fn), true females (tn)]]')\n",
        "print(cf)\n",
        "print('tn, fp, fn, tp')\n",
        "print(tn, fp, fn, tp)\n",
        "\n",
        "n = y_bench.shape[0]\n",
        "tn = cf[0,0]\n",
        "tp = cf[1,1]\n",
        "fp = cf[0,1]\n",
        "fn = cf[1,0]\n",
        "print(tn, fp, fn, tp)\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "\n",
        "print(\"Precision (manual): \",calc_precision(tp,fp))\n",
        "# Only report results for the class specified by pos_label. \n",
        "# This is applicable only if targets (y_{true,pred}) are binary.\n",
        "ps = precision_score(y_true, y_pred, average='binary')\n",
        "print('Precision score sklearn (binary): ', ps)\n",
        "# If None, the scores for each class are returned\n",
        "ps = precision_score(y_true, y_pred, average=None)\n",
        "print('Precision score sklearn (none): ', ps)\n",
        "\n",
        "print(\"Recall (manual): \",calc_recall(tp,fn))\n",
        "rs = recall_score(y_true, y_pred, average='binary')\n",
        "print('Recall score sklearn (binary): ', rs)\n",
        "rs = recall_score(y_true, y_pred, average=None)\n",
        "print('Recall score sklearn (none): ', rs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Confusion matrix for the validation set:')\n",
        "cm_validation = confusion_matrix(y_bench[:,0], p_bench_c[:,0])\n",
        "sns.heatmap(cm_validation, annot=True, fmt='d')\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')"
      ],
      "metadata": {
        "id": "5qLO2kHO-4ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7gOtATL9E1p"
      },
      "source": [
        "<font color='blue'>Task</font>: For 2 images of each class, output the probability of belonging to a class as calculated by the CNN. Select 1 example each that shows clear results and 1 example each that shows uncertain predictions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm1untSld6Ar"
      },
      "outputs": [],
      "source": [
        "def print_prediction(X_, y_,p_,i):\n",
        "  print(\"Label: \",y_[i,:])\n",
        "  print(\"Prediction (class): \", np.round(p_[i,:]))\n",
        "  print(\"Prediction (prob): \",p_[i,:])\n",
        "\n",
        "def show_example_prediction(X_, y_,p_,i):\n",
        "  show_img(X_,i)\n",
        "  print_prediction(X_, y_,p_,i)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "# Hint: the probabilites were already calculated above\n",
        "\n",
        "# females (smaller indices - depends on your total ground truth size)\n",
        "show_example_prediction(X_bench,y_bench,p_bench,0)\n",
        "show_example_prediction(X_bench,y_bench,p_bench,1)\n",
        "\n",
        "# males (larger indices - depends on your total ground truth size)\n",
        "show_example_prediction(X_bench,y_bench,p_bench,110) \n",
        "show_example_prediction(X_bench,y_bench,p_bench,150) \n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check With Your Own Photos"
      ],
      "metadata": {
        "id": "ondx4XoMMlSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to upload your foto in jpg or jpeg format\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "knrOOmV3Mqzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of the uploaded jpeg images\n",
        "image_list_test = glob.glob('*.jp*g')\n",
        "print(image_list_test)"
      ],
      "metadata": {
        "id": "nSAoFelxLZEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the test set\n",
        "X_test2 = []\n",
        "y_test2 = []\n",
        "\n",
        "# Pre-process images and append to the test set\n",
        "img_preprocessing(image_list_test, [], X_test2, y_test2)"
      ],
      "metadata": {
        "id": "Z6henLctMvYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "X_test22 = np.array(X_test2)\n",
        "print(X_test22.shape)\n",
        "\n",
        "X_test23 = X_test22.reshape(X_test22.shape[0], X_test22.shape[1], X_test22.shape[2], 3)\n",
        "\n",
        "# Get the predictions\n",
        "p_test23 = (model.predict(X_test23)>0.5).astype('int32')\n",
        "print(p_test23)"
      ],
      "metadata": {
        "id": "vP6XLJUpMyit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the uploaded images and show predictions\n",
        "for i in range(0,len(image_list_test)):\n",
        "  show_img(X_test23,i)\n",
        "  print(\"Prediction:  \",p_test23[i,:])\n",
        "  print(\"_______________________\")"
      ],
      "metadata": {
        "id": "c8LXpS8PM2Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretable ML Analysis: Grad-CAM\n",
        "Adapted from: https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353"
      ],
      "metadata": {
        "id": "GPOt1A83TX7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"grad_cam\"></a>\n",
        "\n",
        "## Gradient-weighted Class Activation Mapping (Grad-CAM)\n",
        "\n",
        "Grad-CAM ist eine Methode, die Gradienten aus der letzten Schicht eines convolutional Layers eines CNNs extrahiert, um die Regionen auf den Inputbildern visuell hervorzuheben, welche den grössten Einfluss auf die vorhergesagte Wahrscheintlichkeit einer Klasse haben.\n",
        "\n",
        "Links:\n",
        "* __[Grad-CAM Tutorial](https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353)__ (inkl. Link zu Colab Notebook)\n",
        "* __[Github Repository für Grad-CAM mit PyTorch](https://github.com/jacobgil/pytorch-grad-cam)__ (inkl. diversen Erweiterungen zu Grad-CAM)"
      ],
      "metadata": {
        "id": "vnTJLHga4mWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "import cv2\n",
        "\n",
        "def GradCam(model, img_array, layer_name, eps=1e-8):\n",
        "    '''\n",
        "    Creates a grad-cam heatmap given a model and a layer name contained with that model\n",
        "    \n",
        "    Args:\n",
        "      model: tf model\n",
        "      img_array: (img_width x img_width) numpy array\n",
        "      layer_name: str\n",
        "\n",
        "    Returns \n",
        "      uint8 numpy array with shape (img_height, img_width)\n",
        "\n",
        "    '''\n",
        "\n",
        "    gradModel = Model(\n",
        "\t\t\tinputs=[model.inputs],\n",
        "\t\t\toutputs=[model.get_layer(layer_name).output,\n",
        "\t\t\t\tmodel.output])\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "\t\t\t# cast the image tensor to a float-32 data type, pass the\n",
        "\t\t\t# image through the gradient model, and grab the loss\n",
        "\t\t\t# associated with the specific class index\n",
        "            inputs = tf.cast(img_array, tf.float32)\n",
        "            (convOutputs, predictions) = gradModel(inputs)\n",
        "            loss = predictions[:, 0]\n",
        "\t\t# use automatic differentiation to compute the gradients\n",
        "    grads = tape.gradient(loss, convOutputs)\n",
        "    \n",
        "    # compute the guided gradients\n",
        "    castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
        "    castGrads = tf.cast(grads > 0, \"float32\")\n",
        "    guidedGrads = castConvOutputs * castGrads * grads\n",
        "\t\t# the convolution and guided gradients have a batch dimension\n",
        "\t\t# (which we don't need) so let's grab the volume itself and\n",
        "\t\t# discard the batch\n",
        "    convOutputs = convOutputs[0]\n",
        "    guidedGrads = guidedGrads[0]\n",
        "    # compute the average of the gradient values, and using them\n",
        "\t\t# as weights, compute the ponderation of the filters with\n",
        "\t\t# respect to the weights\n",
        "    weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
        "    cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
        "  \n",
        "    # grab the spatial dimensions of the input image and resize\n",
        "\t\t# the output class activation map to match the input image\n",
        "\t\t# dimensions\n",
        "    (w, h) = (img_array.shape[2], img_array.shape[1])\n",
        "    heatmap = cv2.resize(cam.numpy(), (w, h))\n",
        "\t\t# normalize the heatmap such that all values lie in the range\n",
        "\t\t# [0, 1], scale the resulting values to the range [0, 255],\n",
        "\t\t# and then convert to an unsigned 8-bit integer\n",
        "    numer = heatmap - np.min(heatmap)\n",
        "    denom = (heatmap.max() - heatmap.min()) + eps\n",
        "    heatmap = numer / denom\n",
        "    # heatmap = (heatmap * 255).astype(\"uint8\")\n",
        "\t\t# return the resulting heatmap to the calling function\n",
        "    return heatmap\n",
        "\n",
        "\n",
        "def sigmoid(x, a, b, c):\n",
        "    return c / (1 + np.exp(-a * (x-b)))\n",
        "\n",
        "def superimpose(img_bgr, cam, thresh, emphasize=False):\n",
        "    \n",
        "    '''\n",
        "    Superimposes a grad-cam heatmap onto an image for model interpretation and visualization.\n",
        "    \n",
        "    Args:\n",
        "      image: (img_width x img_height x 3) numpy array\n",
        "      grad-cam heatmap: (img_width x img_width) numpy array\n",
        "      threshold: float\n",
        "      emphasize: boolean\n",
        "\n",
        "    Returns \n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\n",
        "\n",
        "    '''\n",
        "    heatmap = cv2.resize(cam, (img_bgr.shape[0], img_bgr.shape[1]))\n",
        "    if emphasize:\n",
        "        heatmap = sigmoid(heatmap, 50, thresh, 1)\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    \n",
        "    hif = .8\n",
        "    superimposed_img = heatmap * hif + img_bgr\n",
        "    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n",
        "    superimposed_img_rgb = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    return superimposed_img_rgb"
      ],
      "metadata": {
        "id": "XtG1iXhDTXLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>Tasks</font>: \n",
        "* Zeige die Grad-CAM Resultate für verschiedene Beispielbilder an (i= )\n",
        "  * Wie verhalten sich die Predictions für unterschiedliche Bilderklassen wie Frauen, Männern, Sportler, Nicht-Sportler etc.\n",
        "* Analysiere und diskutiere die Grad-CAM Resultate der unterschiedlichen CNN-Layers\n",
        "  * Schaue dir die Gradienten Maps der vierschiedenen Blöcke an und interpretiere die Features\n",
        "  * Welche Muster beobachtest du? Welche Layers sind relevant für die Klassifikation?\n",
        "* Kannst du mittels Grad-CAM die Architektur des CNNs verbessern?\n",
        "* Verwende optional andere XAI Methoden wie SHAP zur Analyse"
      ],
      "metadata": {
        "id": "-DA8K1cOd1hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_bench.shape)\n",
        "# Task: select image for analysis\n",
        "i = 3\n",
        "img = X_bench[i, :, :] # Task: play with images\n",
        "print(img.shape)\n",
        "print_prediction(X_bench,y_bench,p_bench,i)\n",
        "\n",
        "## Grad-CAM heatmap for the last convolutional layer in the model\n",
        "\n",
        "# Task: add layer name you want to interpret grad-CAM for\n",
        "# Task: find relevant CNN layers for the classification of the images\n",
        "layer_name1 = 'conv2d' \n",
        "\n",
        "grad_cam = GradCam(model,np.expand_dims(img, axis=0),layer_name1)\n",
        "# Task: optimize parameters for visualization\n",
        "grad_cam_superimposed = superimpose(img, grad_cam, 0.35, emphasize=True)\n",
        "\n",
        "layer_name2 = 'conv2d_1' \n",
        "grad_cam = GradCam(model,np.expand_dims(img, axis=0),layer_name2)\n",
        "# Task: optimize parameters for visualization\n",
        "grad_cam_superimposed2 = superimpose(img, grad_cam, 0.35, emphasize=True)\n",
        "\n",
        "layer_name3 = 'conv2d_2' \n",
        "grad_cam = GradCam(model,np.expand_dims(img, axis=0),layer_name3)\n",
        "# Task: optimize parameters for visualization\n",
        "grad_cam_superimposed3 = superimpose(img, grad_cam, 0.35, emphasize=True)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "ax = plt.subplot(1, 4, 1)\n",
        "plt.imshow(img)\n",
        "plt.title('Original image')\n",
        "ax = plt.subplot(1, 4, 2)\n",
        "plt.imshow(grad_cam_superimposed)\n",
        "plt.title(layer_name1 + ' grad-CAM')\n",
        "ax = plt.subplot(1, 4, 3)\n",
        "plt.imshow(grad_cam_superimposed2)\n",
        "plt.title(layer_name2 + ' grad-CAM')\n",
        "ax = plt.subplot(1, 4, 4)\n",
        "plt.imshow(grad_cam_superimposed3)\n",
        "plt.title(layer_name3 + ' grad-CAM')\n",
        "plt.axis('off')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "yaN1Nx2VUMWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations for completing this exercise!\n"
      ],
      "metadata": {
        "id": "hbhNOx7jM4ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deluxe: Data Augmentation\n",
        "Data augmentation is performed in order to make your ground truth dataset more diverse such that the computed algorithm is more robust to new data - not used for the training. Typical data augmentation modifications include adjusting brightness, color, contrast, distorions, adding noise, zooming, rotations, or mirroring.\n",
        "\n",
        "There are various ready made data augmentor tools available. In case you need to augment also your labels, make sure you choose an augmentor tool that is able to do so. The one presented here can only augment raw images.  \n",
        "\n",
        "<font color='blue'>Optional task</font>: integrate data augmentation into your deep learning model.*kursiver Text*"
      ],
      "metadata": {
        "id": "ALdz_E1oPEEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Augmentor"
      ],
      "metadata": {
        "id": "0jsDc-rOPVSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Augmentor (library to create more distorted images)\n",
        "import Augmentor\n",
        "\n",
        "# Define a function to create augmented images images\n",
        "# Task: change parameters of augmentor\n",
        "def build_augmented_images(path_folder, n_samples):\n",
        "    p = Augmentor.Pipeline(path_folder)\n",
        "    p.rotate(probability=0.3, max_left_rotation=4, max_right_rotation=4)\n",
        "    p.zoom(probability=0.3, min_factor=0.7, max_factor=1.2) \n",
        "    p.random_distortion(probability=0.3, grid_width=4, grid_height=4, magnitude=6)\n",
        "    p.random_brightness(probability=0.8, min_factor=0.5, max_factor=2)\n",
        "    p.random_color(probability=0.3, min_factor=0.5, max_factor=1.5)\n",
        "    p.random_contrast(probability=0.3, min_factor=0.8, max_factor=1.2)\n",
        "\n",
        "    p.sample(n_samples, multi_threaded=False)"
      ],
      "metadata": {
        "id": "X5A021YCPAok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove previous augmented images, if any (in case you run it twice) \n",
        "\n",
        "import shutil\n",
        "\n",
        "f_augmented = os.path.join(f_dir_sel, 'output')\n",
        "m_augmented = os.path.join(m_dir_sel, 'output')\n",
        "\n",
        "if os.path.exists(f_augmented):\n",
        "    shutil.rmtree(f_augmented)\n",
        "    \n",
        "if os.path.exists(m_augmented):\n",
        "    shutil.rmtree(m_augmented)"
      ],
      "metadata": {
        "id": "9o-VN4uCPYAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f_augmented, m_augmented)"
      ],
      "metadata": {
        "id": "Ue_2GHhrQzwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of augmented images\n",
        "\n",
        "n_f = 1000 # number of augmented female images\n",
        "n_m = 1000 # number of augmented male images\n",
        "\n",
        "# Task: choose your own number of augmented images for the two classes\n",
        "\n",
        "# Hint 1: if you start with smaller numbers, the CNN will be trained faster\n",
        "# However, the more images you add, the better accuracy/performace you will achieve\n",
        "# The performance of the CNN can be evaluated in the section \"Prediction Performance\"\n",
        "# Hint 2: consider whether you want to select the same or an unequal number of \n",
        "# images for the two classes\n",
        "\n",
        "# Create augmented images\n",
        "build_augmented_images(f_dir_sel, n_f)\n",
        "build_augmented_images(m_dir_sel, n_m)  "
      ],
      "metadata": {
        "id": "rnwxkIRCPbIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of augmented images of each category\n",
        "\n",
        "print_file_count_in_dir(f_augmented, \"female augmented cases: \")\n",
        "print_file_count_in_dir(m_augmented, \"male augmented cases: \")"
      ],
      "metadata": {
        "id": "rG29an7_Pgfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example augmented images\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from skimage import io\n",
        "import os\n",
        "\n",
        "import fnmatch\n",
        "\n",
        "def show_augmented_images_example(image_type, person):\n",
        "  \n",
        "  base_path = os.path.join(os.path.join('faces', image_type.lower()),'selected')\n",
        "  augmentation_path = os.path.join(base_path, 'output')\n",
        "\n",
        "  pattern = '*' + person + '*.jpg';\n",
        "  original_match = fnmatch.filter(os.listdir(base_path), pattern)\n",
        "  if len(original_match) <= 0:\n",
        "    print(\"select a valid image number or type for: \" + person)\n",
        "    return\n",
        "\n",
        "  original = io.imread(os.path.join(base_path, original_match[0]))\n",
        "  augmented = fnmatch.filter(os.listdir(augmentation_path), pattern)\n",
        "\n",
        "  n_augmented = len(augmented)\n",
        "\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax1 = fig.add_subplot(1,n_augmented+1,1)\n",
        "  ax1.set_title(\"Original\")\n",
        "  ax1.imshow(original)\n",
        "\n",
        "  for a in range(n_augmented):\n",
        "    im_augmented = io.imread(os.path.join(augmentation_path, augmented[a]))\n",
        "    ax = fig.add_subplot(1,n_augmented+1,a+2)\n",
        "    ax.imshow(im_augmented)\n",
        "    ax.set_title(\"Augmented\")\n"
      ],
      "metadata": {
        "id": "nkqAN3MIPhPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $f_augmented"
      ],
      "metadata": {
        "id": "ivLYntojuTef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task: display exemplar augmented images: change image number and image type\n",
        "person = 'Conchita_Martinez_0001' # e.g., Vanessa_Redgrave_0002, Yoriko_Kawaguchi_0004\n",
        "image_type = 'female' # 'female' or 'male'\n",
        "show_augmented_images_example(image_type, person)"
      ],
      "metadata": {
        "id": "TZzQGZq6Pkpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then continue with the section data preparation."
      ],
      "metadata": {
        "id": "Lt4PhRnYPxvc"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}